---
title: "Homework 5"
author: "Charlotte Fowler"
date: "11/8/2019"
output: html_document
---


# Problem 1 
```{r}
#loading packages
library(tidyverse)
```



```{r}
#reading in data
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```



```{r}
#writing function to fill in missing values
missing_values_fix = function(x){
  if (is.numeric(x)) {
    ifelse(is.na(x), mean(x, na.rm = TRUE), x)
  } else if (is.character(x)) {
    ifelse(is.na(x), "virginica", x)
  }
  

}

#performing function on iris data
output = map(iris_with_missing, missing_values_fix)


#creating new data
iris_data = tibble(
  sepal_length = output[[1]], 
  sepal_width = output[[2]],
  petal_length = output[[3]],
  petal_width = output[[4]],
  species = output[[5]]
)
```


# Problem 2 

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

Start with a dataframe containing all file names; the list.files function will help
Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary
Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups


```{r message = FALSE}
#reading in and tidying the data
file_names = list.files("data")

combined_df = map_dfr(.x = file_names, ~ read_csv(str_c("data/",.x)))

combined_df = combined_df %>% 
  mutate(
    id = file_names
  ) %>% 
  separate(id, into = c("group", "id"), sep = "_") %>% 
  mutate(id = str_remove(id, ".csv")) %>% 
  pivot_longer(week_1:week_8, names_to = "week", values_to = "value") %>% 
  mutate(week = str_remove(week, "week_"))
```
Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups

```{r}
#making spaghetti plot
combined_df %>% 
  mutate(unique_id = paste(id, group, sep = "_")) %>% 
  ggplot(aes(y = value, x = week, group = unique_id, color = group)) +
  geom_line() + 
  viridis::scale_color_viridis(
    discrete = TRUE, 
    name = "",
    breaks=c("con", "exp"),
    labels=c("Control", "Experiment")
    ) + 
  theme_bw() + 
  labs(title = "Weekly Obsevations by Participant", y = "Observation", x = "Week") + 
  theme(legend.position = "bottom") 
```

It appears that the control group tended to have lower values than the experiment group, especially as time progressed. 




# Problem 3 

When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a simple linear regression.

First set the following design elements:

Fix n=30
Fix xi1 as draws from a standard Normal distribution
Fix β0=2
Fix σ2=50
Set β1=0. Generate 10000 datasets from the model
yi=β0+β1xi1+ϵi
with ϵi∼N[0,σ2]. For each dataset, save β̂ 1 and the p-value arising from a test of H:β1=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of lm.

```{r}
#writing function to run regression 
sim_regression = function(beta1, n = 30, beta0 = 2) {
  
  sim_data = tibble(
    x = rnorm(n),
    y = beta0 + beta1 * x + rnorm(n, 0, sqrt(50)) 
  )
  
  ls_fit = broom::tidy(lm(y ~ x, data = sim_data))
  
  tibble(
    beta1_hat = pull(ls_fit, estimate)[2],
    beta1_hat_pvalue = pull(ls_fit, p.value)[2]
  )
}
```

```{r}
#running simulation 10000 for beta1 = 0 
output = vector("list", 10000)

for (i in 1:10000) {
  output[[i]] = sim_regression(0)
}

results_b1_0 = bind_rows(output)
```

Repeat the above for β1={1,2,3,4,5,6}


```{r}
#running sim for beta1 ={0,1,2,3,4,5,6}
sim_results = 
  tibble(beta1 = 0:6) %>% 
  mutate(
    output_lists = map(.x = beta1, ~rerun(10000, sim_regression(beta1 = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```


```{r}
#plotting proportion of rejections by beta1
sim_results %>% 
  group_by(beta1) %>% 
  filter(beta1_hat_pvalue<0.05) %>% 
  count() %>% 
  mutate(prop = n/10000) %>% 
  ggplot(aes(y = prop, x = beta1)) +
  geom_line() +
  geom_point() + 
  labs(title = "Proportion of null rejections by true Beta 1 value", y = "Proportion") + 
  theme_bw() +
  scale_x_continuous(name="Beta 1", breaks=0:6) 
```

As effect size ($\beta_1$) increases, the probability we reject the null, and therefore the power increases significantly. By the time $\beta_1 = 6$ given the experiment conditions, we are almost certain to reject the null. 



```{r}
all_beta_avgs = sim_results %>% 
  group_by(beta1) %>% 
  summarise(all_beta1 = mean(beta1_hat))  

sig_beta_avgs = sim_results %>% 
  filter(beta1_hat_pvalue<0.05) %>% 
  group_by(beta1) %>% 
  summarise(sig_beta1 = mean(beta1_hat))  
  

left_join(all_beta_avgs, sig_beta_avgs, by = "beta1") %>% 
  pivot_longer(cols = c(all_beta1, sig_beta1), names_to = "type", values_to = "avg") %>% 
  ggplot(aes(x = beta1, y = avg, color = type)) + 
  geom_line() +
  geom_point() + 
  labs(title = "Average approximate beta by true beta value") + 
  viridis::scale_color_viridis(
    discrete = TRUE, 
    name = "",
    breaks=c("all_beta1", "sig_beta1"),
    labels=c("all beta", "significant only beta")) + 
  theme_bw() +
  scale_x_continuous(name="true beta 1", breaks=0:6) + 
  scale_y_continuous(name="average beta 1 hat", breaks=0:6) + 
  theme(legend.position = "bottom") 
  

```

